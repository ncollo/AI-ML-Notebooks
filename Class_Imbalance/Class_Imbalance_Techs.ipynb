{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Imbalanced Datasets\n",
        "This notebook demonstrated baseline model and four approaches to handle imbalanced data for Spam vs Ham classification.<br>\n",
        "The notebook provides a step-by-step process, from dataset loading, exploration, and preprocessing, to training models using several imbalanced data handling techniques with performance evaluation after each. It uses the classic SMS Spam Collection dataset, which is a popular benchmark for spam detection tasks with class imbalance.Here is a comprehensive Python Jupyter notebook demonstrating various imbalanced data handling techniques on the spam vs ham dataset. It covers dataset loading, exploration, preprocessing, train-test splitting, baseline model training, and handling imbalance with undersampling, oversampling, SMOTE, and class weighting, with evaluation after each step:"
      ],
      "metadata": {
        "id": "oJSN5rIxJmxA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0YjkZ-bJlLR"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset (example dataset - SMS Spam Collection dataset)\n",
        "# URL for dataset: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
        "# or usually saved locally as 'spam.csv'\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
        "df.columns = ['label', 'message']\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Dataset sample:\")\n",
        "display(df.head())\n",
        "\n",
        "# Explore and visualize class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title(\"Spam vs Ham Class Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Encode labels: ham=0, spam=1\n",
        "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Text preprocessing - basic (can be extended)\n",
        "# For simplicity, just lowercase here\n",
        "df['message'] = df['message'].str.lower()\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['message'], df['label_num'], test_size=0.3, random_state=42, stratify=df['label_num'])\n",
        "\n",
        "print(f\"\\nTrain set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_test, y_test, title='Confusion Matrix'):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Ham', 'Spam'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Baseline model on imbalanced data\n",
        "print(\"Baseline Model (Imbalanced Data):\")\n",
        "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "baseline_model.fit(X_train_tfidf, y_train)\n",
        "evaluate_model(baseline_model, X_test_tfidf, y_test, 'Baseline Confusion Matrix')\n",
        "\n",
        "# Show training label distribution\n",
        "print(\"\\nTraining label distribution before resampling:\")\n",
        "print(Counter(y_train))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling imbalance techniques\n",
        "\n",
        "# 1. Random Undersampling majority class\n",
        "print(\"\\n1. Random Undersampling:\")\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_rus, y_rus = rus.fit_resample(X_train_tfidf, y_train)\n",
        "print(\"Resampled label distribution:\", Counter(y_rus))\n",
        "rus_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "rus_model.fit(X_rus, y_rus)\n",
        "evaluate_model(rus_model, X_test_tfidf, y_test, 'Random Undersampling Confusion Matrix')\n",
        "\n",
        "# 2. Random Oversampling minority class\n",
        "print(\"\\n2. Random Oversampling:\")\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_ros, y_ros = ros.fit_resample(X_train_tfidf, y_train)\n",
        "print(\"Resampled label distribution:\", Counter(y_ros))\n",
        "ros_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "ros_model.fit(X_ros, y_ros)\n",
        "evaluate_model(ros_model, X_test_tfidf, y_test, 'Random Oversampling Confusion Matrix')\n",
        "\n",
        "# 3. SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "print(\"\\n3. SMOTE Oversampling:\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
        "print(\"Resampled label distribution:\", Counter(y_smote))\n",
        "smote_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "smote_model.fit(X_smote, y_smote)\n",
        "evaluate_model(smote_model, X_test_tfidf, y_test, 'SMOTE Confusion Matrix')\n",
        "\n",
        "# 4. Class weighting in Logistic Regression\n",
        "print(\"\\n4. Class Weighting:\")\n",
        "class_weight_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "class_weight_model.fit(X_train_tfidf, y_train)\n",
        "evaluate_model(class_weight_model, X_test_tfidf, y_test, 'Class Weighting Confusion Matrix')\n",
        "\n",
        "# Summary\n",
        "print(\"\\nSummary:\")\n",
        "print(\"This notebook demonstrated baseline model and four approaches to handle imbalanced data for Spam vs Ham classification:\")\n",
        "print(\"- Random Undersampling\")\n",
        "print(\"- Random Oversampling\")\n",
        "print(\"- SMOTE Oversampling\")\n",
        "print(\"- Class Weighting in model training\")\n",
        "print(\"Choose the technique depending on specific dataset needs and constraints.\")"
      ],
      "metadata": {
        "id": "08f5e892KKDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}